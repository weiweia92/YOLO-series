## YOLOv3  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-05%2014-35-10.png)
YOLO3主要的改进有：调整了网络结构使用了Darknet-53；利用多尺度特征进行对象检测；对象分类用Logistic取代了softmax.  
### Darknet-53  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-05%2014-35-18.png)   
### anchor  
YOLOv2已经开始采用K-means聚类得到先验框的尺寸，YOLOv3延续了这种方法，为每种下采样尺度设定3种先验框，总共聚类出9种尺寸的先验框。在COCO数据集这9个先验框是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。  

分配上，在最小的13*13特征图上（有最大的感受野）应用较大的先验框(116x90)，(156x198)，(373x326)，适合检测较大的对象。中等的26*26特征图上（中等感受野）应用中等的先验框(30x61)，(62x45)，(59x119)，适合检测中等大小的对象。较大的52*52特征图上（较小的感受野）应用较小的先验框(10x13)，(16x30)，(33x23)，适合检测较小的对象。  
感受一下9种先验框的尺寸，下图中蓝色框为聚类得到的先验框。黄色框式ground truth，红框是对象中心点所在的网格.  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-05%2014-35-32.png)
### sigmoid  
预测对象类别时不使用softmax，改成使用logistic的输出进行预测。这样能够支持多标签对象，这样每个类的输出仍是[0,1]之间的一个值，但是他们的和不再是1。只要置信度大于阈值，该锚点便被作为检测框输出。
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-05%2014-35-40.png)  
对于一个416\*416的输入图像，YOLOv3在每个尺度的特征图的每个网格设置3个先验框，总共有 13\*13\*3 + 26\*26\*3 + 52\*52\*3 = 10647 个预测。每一个预测是一个(4+1+80)=85维向量，这个85维向量包含边框坐标（4个数值），边框置信度（1个数值），对象类别的概率（对于COCO数据集，有80种对象）。对比一下，YOLO2采用13\*13\*5 = 845个预测，YOLO3的尝试预测边框数量增加了10多倍，而且是在不同分辨率上进行，所以mAP以及对小物体的检测效果有一定的提升。
